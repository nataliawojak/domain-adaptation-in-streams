{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1682b62e-2cb4-441f-9b1c-895dba46c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 10:19:12.693859: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-13 10:19:13.468362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from focal_loss import BinaryFocalLoss\n",
    "import shap\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, recall_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "from statsmodels.stats.contingency_tables import mcnemar \n",
    "\n",
    "from adapt.feature_based import DANN, ADDA, WDGRL, CCSA, CDAN, MCD, MDD\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential,Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape,Layer, Dropout, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l1,l2\n",
    "import keras.backend as K\n",
    "\n",
    "from river.drift import PageHinkley\n",
    "from river.tree import HoeffdingTreeClassifier, HoeffdingAdaptiveTreeClassifier\n",
    "from river import stream\n",
    "from river import compose\n",
    "from river.datasets import synth\n",
    "from river import ensemble\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import neighbors\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics  import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy.stats import wilcoxon, kendalltau\n",
    "from sklearn.metrics import ndcg_score, dcg_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = 50\n",
    "colors = ['#1e88e5', '#f52757']  # Pink to Blue\n",
    "n_bins = 50  # Number of bins in the colormap\n",
    "shap_colormap = LinearSegmentedColormap.from_list(\"pink_to_blue\", colors, N=n_bins)\n",
    "plt.rc('axes', axisbelow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "675b4ad4-b3f4-4fb0-8199-bc1620d3200b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noisePercentage=0.1\\nnumberOfSamples = 1000\\ndatasets_names = [\"SEA_A\", \"SEA_G\", \"HYPER_A\", \"HYPER_G\"]\\nall_synthetic = {}\\n\\nsea_abrupt = synth.ConceptDriftStream(\\n                stream=synth.SEA(variant=0),\\n                drift_stream=synth.SEA(variant=1, noise=noisePercentage),\\n                position=int(numberOfSamples / 2), width=3)\\n\\nsea_gradual= synth.ConceptDriftStream(\\n                stream=synth.SEA(variant=1),\\n                drift_stream=synth.SEA(variant=2, noise=noisePercentage),\\n                position=int(numberOfSamples / 2), width=0.1*numberOfSamples)\\n\\n\\nhyperplane_abrupt = synth.ConceptDriftStream(\\n                stream=synth.Hyperplane(n_features = 4),\\n                drift_stream=synth.Hyperplane(n_features = 4, n_drift_features = 2, noise_percentage=noisePercentage),\\n                position=int(numberOfSamples / 2), width=3)\\n\\nhyperplane_gradual = synth.ConceptDriftStream(\\n                stream=synth.Hyperplane(n_features = 4),\\n                drift_stream=synth.Hyperplane(n_features = 4, n_drift_features =2, noise_percentage=noisePercentage),\\n                position=int(numberOfSamples / 2), width=0.1*numberOfSamples)\\n\\nfor i, dataset in enumerate([sea_abrupt, sea_gradual, hyperplane_abrupt, hyperplane_gradual]):\\n    data = []\\n    labels = []\\n    \\n    for x, y in dataset.take(numberOfSamples):\\n        data.append(list(x.values()))\\n        labels.append(y)\\n    \\n    df = pd.DataFrame(data, columns=list(x.keys()))\\n    df[\\'class\\'] = labels\\n    df.to_csv(f\"synthetic/{datasets_names[i]}.csv\", index=False)\\n    all_synthetic[datasets_names[i]] = df'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#synthetic dataset generating\n",
    "\n",
    "\"\"\"noisePercentage=0.1\n",
    "numberOfSamples = 1000\n",
    "datasets_names = [\"SEA_A\", \"SEA_G\", \"HYPER_A\", \"HYPER_G\"]\n",
    "all_synthetic = {}\n",
    "\n",
    "sea_abrupt = synth.ConceptDriftStream(\n",
    "                stream=synth.SEA(variant=0),\n",
    "                drift_stream=synth.SEA(variant=1, noise=noisePercentage),\n",
    "                position=int(numberOfSamples / 2), width=3)\n",
    "\n",
    "sea_gradual= synth.ConceptDriftStream(\n",
    "                stream=synth.SEA(variant=1),\n",
    "                drift_stream=synth.SEA(variant=2, noise=noisePercentage),\n",
    "                position=int(numberOfSamples / 2), width=0.1*numberOfSamples)\n",
    "\n",
    "\n",
    "hyperplane_abrupt = synth.ConceptDriftStream(\n",
    "                stream=synth.Hyperplane(n_features = 4),\n",
    "                drift_stream=synth.Hyperplane(n_features = 4, n_drift_features = 2, noise_percentage=noisePercentage),\n",
    "                position=int(numberOfSamples / 2), width=3)\n",
    "\n",
    "hyperplane_gradual = synth.ConceptDriftStream(\n",
    "                stream=synth.Hyperplane(n_features = 4),\n",
    "                drift_stream=synth.Hyperplane(n_features = 4, n_drift_features =2, noise_percentage=noisePercentage),\n",
    "                position=int(numberOfSamples / 2), width=0.1*numberOfSamples)\n",
    "\n",
    "for i, dataset in enumerate([sea_abrupt, sea_gradual, hyperplane_abrupt, hyperplane_gradual]):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for x, y in dataset.take(numberOfSamples):\n",
    "        data.append(list(x.values()))\n",
    "        labels.append(y)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=list(x.keys()))\n",
    "    df['class'] = labels\n",
    "    df.to_csv(f\"synthetic/{datasets_names[i]}.csv\", index=False)\n",
    "    all_synthetic[datasets_names[i]] = df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33106da0-64b4-42e3-8174-67fa2318209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_learning(X: pd.DataFrame,\n",
    "                       model_1 = HoeffdingTreeClassifier(),\n",
    "                       model_2 = HoeffdingAdaptiveTreeClassifier(),\n",
    "                       model_3 = ensemble.ADWINBaggingClassifier(model=linear_model.LogisticRegression()),\n",
    "                       model_4 = neighbors.KNNClassifier(),\n",
    "                       x_cols = [\"x\", \"y\"],\n",
    "                       y_col = [\"class\"]\n",
    "                       ):\n",
    "    all_predictions_HTC = []\n",
    "    all_predictions_HATC = []\n",
    "    all_predictions_adwin = []\n",
    "    all_predictions_knn = []\n",
    "    x = X[x_cols]\n",
    "    y = X[y_col]\n",
    "\n",
    "    model_1 = compose.Pipeline(\n",
    "    ('tree', model_1))\n",
    "    model_2 = compose.Pipeline(\n",
    "    ('tree', model_2))\n",
    "    model_3 = compose.Pipeline(\n",
    "    (\"adwin\", model_3))\n",
    "    model_4 = compose.Pipeline(\n",
    "    (\"knn\", model_4))\n",
    "    \n",
    "    for xi, yi in stream.iter_pandas(x, y):\n",
    "\n",
    "      #classical hoeffding tree classifier  \n",
    "      y_pred = model_1.predict_one(xi)\n",
    "      all_predictions_HTC.append(y_pred)\n",
    "      model_1.learn_one(xi, yi[y_col[0]])\n",
    "\n",
    "      #ensable hoeffding tree classifier  \n",
    "      y_pred = model_2.predict_one(xi)\n",
    "      all_predictions_HATC.append(y_pred)\n",
    "      model_2.learn_one(xi, yi[y_col[0]])\n",
    "\n",
    "      #adwin\n",
    "      y_pred = model_3.predict_one(xi)\n",
    "      all_predictions_adwin.append(y_pred)\n",
    "      model_3.learn_one(xi, yi[y_col[0]])\n",
    "\n",
    "      #knn\n",
    "      y_pred = model_4.predict_one(xi)\n",
    "      all_predictions_knn.append(y_pred)\n",
    "      model_4.learn_one(xi, yi[y_col[0]])\n",
    "        \n",
    "    return all_predictions_HTC, all_predictions_HATC, all_predictions_adwin, all_predictions_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cbaff0f-a8b4-4ea2-ab3c-bc944ce6d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_domain_adaptation(X:pd.DataFrame,\n",
    "                             ccsa_enc,\n",
    "                             ccsa_task,\n",
    "                             ccsa_disc,\n",
    "                             all_predictions_stream_HTC,\n",
    "                             all_predictions_stream_HATC,\n",
    "                             all_predictions_adwin,\n",
    "                             all_predictions_knn,    \n",
    "                             stream_change,\n",
    "                             batch_size,      \n",
    "                             stream_col=\"cluster\",\n",
    "                             x_cols = [\"x\", \"y\"],\n",
    "                             y_col = [\"class\"]):\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  print(\"---------------STREAMING LEARNING---------------\")\n",
    "  #all_predictions_stream, stream_change = streaming_learning(X, delta, x_cols, y_col)\n",
    "\n",
    "  all_results_da_incremental = pd.DataFrame()\n",
    "  all_results_da_simple = pd.DataFrame()\n",
    "  result_info = pd.DataFrame()\n",
    "  df_pred_da = pd.DataFrame()\n",
    "  sv_previous = []\n",
    "    \n",
    "  split_X = split_dataframes_based_on_drift(X, stream_change)\n",
    "  data_source = split_X[0]\n",
    "  print(f\"CHANGES: {stream_change}\")\n",
    "  Xs = data_source[x_cols]\n",
    "  ys = data_source[y_col].values\n",
    "  training = 0\n",
    "\n",
    "\n",
    "  stream_change.append(len(X))\n",
    "\n",
    "  print(\"--------------- DOMAIN ADAPTATION - BATCHES ---------------\")\n",
    "\n",
    "\n",
    "  for i in range(0, len(split_X)-1):\n",
    "    \n",
    "    data_target = split_X[i+1]\n",
    "    Xt = data_target[x_cols]\n",
    "    yt = data_target[y_col].values\n",
    "    start_batch = stream_change[i]\n",
    "    end_batch = start_batch + batch_size\n",
    "    next_stream_change = stream_change[i+1]\n",
    "    batch_training = 0  \n",
    "    f1_source = 0\n",
    "    da_retraining = 0\n",
    "\n",
    "    print(f\"ITERATION : {i}\")\n",
    "    print(f\"START_BATCH: {start_batch}, END_BATCH: {end_batch}, NEXT_CHANGE: {next_stream_change}\")\n",
    "\n",
    "\n",
    "    while end_batch < next_stream_change:\n",
    "\n",
    "      Xt_batch = X[x_cols][start_batch:end_batch]\n",
    "      yt_batch = X[y_col][start_batch:end_batch].values\n",
    "      EPOCHS = 300\n",
    "      INIT_LR = 0.001 \n",
    "      decay=INIT_LR / EPOCHS\n",
    "      model = CCSA(ccsa_enc, ccsa_task, Xt=Xt_batch, yt=yt_batch, metrics=[\"acc\"], \n",
    "               loss=BinaryFocalCrossentropy(gamma=1,  apply_class_balancing=False, alpha=0.25), \n",
    "               random_state=0, optimizer=Adam(learning_rate=INIT_LR, clipvalue=0.5))\n",
    "\n",
    "      model.compile()\n",
    "      f1_source = 0\n",
    "      da_retraining = 0\n",
    "\n",
    "\n",
    "      while (f1_source < 0.5 and (da_retraining<5)):\n",
    "          model.fit(X=Xs, y=ys, Xt=Xt_batch, yt=yt_batch, epochs=EPOCHS, batch_size=128, verbose=0)\n",
    "          y_predict_source = model.predict(Xs)[:,0]\n",
    "          y_predict_source = list(map(lambda x: 1 if x>0.5 else 0, y_predict_source))\n",
    "          f1_source = f1_score(y_predict_source, ys)\n",
    "          da_retraining += 1\n",
    "     \n",
    "     \n",
    "      training = training + 1\n",
    "    \n",
    "      next_batch = end_batch + batch_size\n",
    "\n",
    "      if next_batch < next_stream_change:\n",
    "\n",
    "        \n",
    "\n",
    "        Xt_next_batch = X[x_cols][end_batch:next_batch]\n",
    "        yt_next_batch = X[y_col][end_batch:next_batch].values\n",
    "        y_predict_next_batch = model.predict(Xt_next_batch)[:,0]\n",
    "        y_predict_next_batch = list(map(lambda x: 1 if x>0.5 else 0, y_predict_next_batch))\n",
    "        df_pred = pd.DataFrame(data=y_predict_next_batch, columns=[\"y_pred_da\"], index=Xt_next_batch.index)\n",
    "        df_pred_da = pd.concat([df_pred_da, df_pred])\n",
    "        y_pred_stream_HTC = all_predictions_stream_HTC[end_batch:next_batch]\n",
    "        y_pred_stream_HATC = all_predictions_stream_HATC[end_batch:next_batch]\n",
    "        y_pred_stream_knn = all_predictions_knn[end_batch:next_batch]\n",
    "        y_pred_stream_adwin =  all_predictions_adwin[end_batch:next_batch]\n",
    "\n",
    "    \n",
    "\n",
    "        df_result = pd.DataFrame(data={\n",
    "                          \"data_source\" : ['1'],\n",
    "                          \"data_target\" : [f\"{i+2}\"],\n",
    "                          \"start_batch_fit\" : [start_batch],\n",
    "                          \"end_batch_fit\" : [end_batch],\n",
    "                          \"start_batch_predict\" : [end_batch],\n",
    "                          \"end_batch_predict\" : [end_batch + len(y_predict_next_batch)],\n",
    "                          \"len batch predicted\" :  [len(y_predict_next_batch)],\n",
    "                          \"f1_da\" : [f1_score(yt_next_batch, y_predict_next_batch, average='macro')],\n",
    "                          \"f1_HTC\" : [f1_score(yt_next_batch, y_pred_stream_HTC, average='macro')],\n",
    "                          \"f1_HATC\" : [f1_score(yt_next_batch, y_pred_stream_HATC, average='macro')],\n",
    "                          \"f1_knn\" : [f1_score(yt_next_batch, y_pred_stream_knn, average='macro')],\n",
    "                          \"f1_adwin\" : [f1_score(yt_next_batch, y_pred_stream_adwin, average='macro')],\n",
    "                          \"G_da\" : [geometric_mean_score(yt_next_batch, y_predict_next_batch, average='binary')],\n",
    "                          \"G_HTC\" : [geometric_mean_score(yt_next_batch, y_pred_stream_HTC, average='binary')],\n",
    "                          \"G_HATC\" : [geometric_mean_score(yt_next_batch, y_pred_stream_HATC, average='binary')],\n",
    "                          \"G_knn\" : [geometric_mean_score(yt_next_batch, y_pred_stream_knn, average='binary')],\n",
    "                          \"G_adwin\" : [geometric_mean_score(yt_next_batch, y_pred_stream_adwin, average='binary')],\n",
    "                          })\n",
    "\n",
    "        result_info = pd.concat([result_info, df_result])\n",
    "\n",
    "        end_batch = next_batch\n",
    "\n",
    "      else:\n",
    "        \n",
    "        Xt_next_batch = X[x_cols][end_batch:next_stream_change]\n",
    "        yt_next_batch = X[y_col][end_batch:next_stream_change].values\n",
    "        y_predict_next_batch= model.predict(Xt_next_batch)[:,0]\n",
    "       \n",
    "        y_predict_next_batch = list(map(lambda x: 1 if x>0.5 else 0, y_predict_next_batch))\n",
    "        df_pred = pd.DataFrame(data=y_predict_next_batch, columns=[\"y_pred_da\"], index=Xt_next_batch.index)\n",
    "        df_pred_da = pd.concat([df_pred_da, df_pred])\n",
    "        y_pred_stream_HTC = all_predictions_stream_HTC[end_batch:next_stream_change]\n",
    "        y_pred_stream_HATC = all_predictions_stream_HATC[end_batch:next_stream_change]\n",
    "        y_pred_stream_knn = all_predictions_knn[end_batch:next_stream_change]\n",
    "        y_pred_stream_adwin =  all_predictions_adwin[end_batch:next_stream_change]\n",
    "          \n",
    "\n",
    "        df_result = pd.DataFrame(data={\n",
    "                          \"data_source\" : ['1'],\n",
    "                          \"data_target\" : [f\"{i+2}\"],\n",
    "                          \"start_batch_fit\" : [start_batch],\n",
    "                          \"end_batch_fit\" : [end_batch],\n",
    "                          \"start_batch_predict\" : [end_batch],\n",
    "                          \"end_batch_predict\" : [end_batch + len(y_predict_next_batch)],\n",
    "                          \"len batch predicted\" :  [len(y_predict_next_batch)],\n",
    "                          \"f1_da\" : [f1_score(yt_next_batch, y_predict_next_batch, average='macro')],\n",
    "                          \"f1_HTC\" : [f1_score(yt_next_batch, y_pred_stream_HTC, average='macro')],\n",
    "                          \"f1_HATC\" : [f1_score(yt_next_batch, y_pred_stream_HATC, average='macro')],\n",
    "                          \"f1_knn\" : [f1_score(yt_next_batch, y_pred_stream_knn, average='macro')],\n",
    "                          \"f1_adwin\" : [f1_score(yt_next_batch, y_pred_stream_adwin, average='macro')],\n",
    "                          \"G_da\" : [geometric_mean_score(yt_next_batch, y_predict_next_batch, average='binary')],\n",
    "                          \"G_HTC\" : [geometric_mean_score(yt_next_batch, y_pred_stream_HTC, average='binary')],\n",
    "                          \"G_HATC\" : [geometric_mean_score(yt_next_batch, y_pred_stream_HATC, average='binary')],\n",
    "                          \"G_knn\" : [geometric_mean_score(yt_next_batch, y_pred_stream_knn, average='binary')],\n",
    "                          \"G_adwin\" : [geometric_mean_score(yt_next_batch, y_pred_stream_adwin, average='binary')]\n",
    "                          })\n",
    "\n",
    "        result_info = pd.concat([result_info, df_result])\n",
    "        break\n",
    "\n",
    "    \n",
    "    df_pred_stream_HTC = pd.DataFrame(data=all_predictions_stream_HTC, columns=[\"y_pred_stream_HTC\"], index=X.index)\n",
    "    df_pred_stream_HATC = pd.DataFrame(data=all_predictions_stream_HATC, columns=[\"y_pred_stream_HATC\"], index=X.index)\n",
    "    df_pred_stream_knn = pd.DataFrame(data=all_predictions_knn, columns=[\"y_pred_stream_knn\"], index=X.index)\n",
    "    df_pred_stream_adwin = pd.DataFrame(data=all_predictions_adwin, columns=[\"y_pred_stream_adwin\"], index=X.index)\n",
    "    df_true = pd.DataFrame(data=X[y_col].values, columns=[\"y_true\"], index=X.index)\n",
    "    df_da = pd.merge(df_pred_da, df_pred_stream_HTC, left_index=True, right_index=True)\n",
    "    df_da = pd.merge(df_da, df_pred_stream_HATC, left_index=True, right_index=True)\n",
    "    df_da = pd.merge(df_da, df_pred_stream_knn, left_index=True, right_index=True)\n",
    "    df_da = pd.merge(df_da, df_pred_stream_adwin, left_index=True, right_index=True)\n",
    "    df_stream_da = pd.merge(df_da, df_true, left_index=True, right_index=True)\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "  return result_info, df_stream_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60034d3-d141-45b3-912d-af9105af698c",
   "metadata": {},
   "source": [
    "## Test the CCSA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf991ba-c961-4f8d-9c62-4cf46e60e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframes_based_on_drift(X: pd.DataFrame,\n",
    "                                    changes: list):\n",
    "  list_of_change = changes.copy()\n",
    "  split_dataframes = []\n",
    "  start_index = 0\n",
    "  end_index = 0\n",
    "  list_of_change.append(len(X))\n",
    "  if len(changes) < 1:\n",
    "    return X\n",
    "  else:\n",
    "    for i in range(len(list_of_change)):\n",
    "      end_index = list_of_change[i]\n",
    "      split_dataframes.append(X[start_index:end_index])\n",
    "      start_index=end_index\n",
    "    return split_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10ff764-f247-4ea7-9114-fb2da32b3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_in_domains(stream_change, df_stream):\n",
    "    result_info = pd.DataFrame()\n",
    "    for i in range(len(stream_change)-1):\n",
    "        start_stream = stream_change[i]\n",
    "        end_stream = stream_change[i+1]\n",
    "        analysed_stream = df_stream[df_stream.index>start_stream]\n",
    "        analysed_stream = analysed_stream[analysed_stream.index<end_stream]\n",
    "        true = analysed_stream[\"y_true\"]\n",
    "        pred = analysed_stream[\"y_pred_da\"]\n",
    "        stats = pd.DataFrame(data={\n",
    "                              \"data_source\" : ['1'],\n",
    "                              \"data_target\" : [f\"{i+2}\"],\n",
    "                              \"start_domain\" : [start_stream],\n",
    "                              \"end_domain\": [end_stream],\n",
    "                              \"len batch\" :  [len(analysed_stream)],\n",
    "                              \"f1_da\" : [f1_score(true, pred, average='macro')],\n",
    "                              \"G_da\" : [geometric_mean_score(true, pred)],\n",
    "                              \"accuracy_da\" : [accuracy_score(true, pred)],\n",
    "                              \"precision_da\" : [precision_score(true, pred, average='macro')],\n",
    "                              \"recall_da\" : [recall_score(true, pred, average='macro')],\n",
    "                              })\n",
    "        result_info = pd.concat([result_info, stats])\n",
    "    result_info = result_info.reset_index(drop=True)\n",
    "    return result_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6997e64b-661d-49a5-be1c-e0127e74f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream_filepath = f\"{filepath}/df_stream_da_monday_v1.csv\"\n",
    "#df_stream = pd.read_csv(stream_filepath, index_col=0)\n",
    "#change_filepath = f\"{filepath}/stream_change_monday_v1.csv\"\n",
    "#stream_change = pd.read_csv(change_filepath, index_col=0)[\"0\"].values\n",
    "#stats = stats_in_domains(stream_change = stream_change, df_stream = df_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af0bf6e-ba51-4f11-b91e-81bd39da7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLdivergence(x, y):\n",
    "  \"\"\"Compute the Kullback-Leibler divergence between two multivariate samples.\n",
    "  Parameters\n",
    "  ----------\n",
    "  x : 2D array (n,d)\n",
    "    Samples from distribution P, which typically represents the true\n",
    "    distribution.\n",
    "  y : 2D array (m,d)\n",
    "    Samples from distribution Q, which typically represents the approximate\n",
    "    distribution.\n",
    "  Returns\n",
    "  -------\n",
    "  out : float\n",
    "    The estimated Kullback-Leibler divergence D(P||Q).\n",
    "  References\n",
    "  ----------\n",
    "  Pérez-Cruz, F. Kullback-Leibler divergence estimation of\n",
    "continuous distributions IEEE International Symposium on Information\n",
    "Theory, 2008.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Check the dimensions are consistent\n",
    "  x = np.atleast_2d(x)\n",
    "  y = np.atleast_2d(y)\n",
    "\n",
    "  n,d = x.shape\n",
    "  m,dy = y.shape\n",
    "\n",
    "  assert(d == dy)\n",
    "\n",
    "\n",
    "  # Build a KD tree representation of the samples and find the nearest neighbour\n",
    "  # of each point in x.\n",
    "  xtree = KDTree(x)\n",
    "  ytree = KDTree(y)\n",
    "\n",
    "  # Get the first two nearest neighbours for x, since the closest one is the\n",
    "  # sample itself.\n",
    "  r = xtree.query(x, k=2, eps=.01, p=2)[0][:,1]\n",
    "  s = ytree.query(x, k=1, eps=.01, p=2)[0]\n",
    "\n",
    "  # There is a mistake in the paper. In Eq. 14, the right side misses a negative sign\n",
    "  # on the first term of the right hand side.\n",
    "  log_ = -np.log(r/s)\n",
    "  log_= log_[np.isfinite(log_)]\n",
    "    \n",
    "  return log_.sum() * d / n + np.log(m / (n - 1.))\n",
    "  #return r, s, -np.log(r/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b72cbae-90fb-4ff0-9e7a-7fc38fb1b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_drift_detection(X: pd.DataFrame,\n",
    "                                 referance_samples = 100, \n",
    "                                 x_cols = [\"x\", \"y\"],\n",
    "                                 min_instances=10,\n",
    "                                 samples_to_check = 1,\n",
    "                                 delta=0.05,\n",
    "                                 threshold=4,\n",
    "                                 alpha=0.9999,\n",
    "                                 save=False\n",
    "                                 \n",
    "                                ):\n",
    "\n",
    "    reference_distribution = X[x_cols][:referance_samples].values \n",
    "    X[\"KL\"] = -1\n",
    "    stream_change = []\n",
    "    ph = PageHinkley(min_instances=min_instances, delta=delta, threshold=threshold, alpha=alpha)\n",
    "    start_drift = 2\n",
    "    referance_indx_start = referance_indx_end =  0\n",
    "    \n",
    "    for i in range(referance_samples, len(X)-samples_to_check, samples_to_check):\n",
    "\n",
    "        sample =  X[x_cols].loc[i:i+samples_to_check].values \n",
    "        \n",
    "        kl_divergence = KLdivergence(reference_distribution, sample)\n",
    "        X[\"KL\"].loc[i] = kl_divergence\n",
    "        ph.update(kl_divergence)\n",
    "        if ph.drift_detected:\n",
    "          #print(\"detected drift!\")\n",
    "          stream_change.append(i)\n",
    "          start_drift = i\n",
    "          reference_distribution = X[x_cols][start_drift-2:i].values\n",
    "\n",
    "        else:\n",
    "            reference_distribution = X[x_cols][start_drift-2:i].values\n",
    "          \n",
    "\n",
    "    return stream_change, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec51b50-2f48-49fa-b8bb-05783bec0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEA_A = pd.read_csv(\"synthetic/SEA_A.csv\")\n",
    "SEA_G = pd.read_csv(\"synthetic/SEA_G.csv\")\n",
    "HYPER_A = pd.read_csv(\"synthetic/HYPER_A.csv\")\n",
    "HYPER_G = pd.read_csv(\"synthetic/HYPER_G.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "040c7471-9e42-4518-8389-122fc96b7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = [\"SEA_A\", \"SEA_G\", \"HYPER_A\", \"HYPER_G\"]\n",
    "all_synthetic = {}\n",
    "all_synthetic[\"SEA_A\"] = SEA_A\n",
    "all_synthetic[\"SEA_G\"] = SEA_G\n",
    "all_synthetic[\"HYPER_A\"] = HYPER_A\n",
    "all_synthetic[\"HYPER_G\"] = HYPER_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f7202c-8670-4776-ba0a-60b7f623f605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEA_A\n",
      "['0' '1' '2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 10:19:15.915035: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-02-13 10:19:15.915064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: 002bfeb9166b\n",
      "2025-02-13 10:19:15.915071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: 002bfeb9166b\n",
      "2025-02-13 10:19:15.915318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 550.90.7\n",
      "2025-02-13 10:19:15.915339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 550.90.7\n",
      "2025-02-13 10:19:15.915344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 550.90.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------STREAMING LEARNING---------------\n",
      "CHANGES: [230, 302, 358, 416, 444, 578, 654, 816]\n",
      "--------------- DOMAIN ADAPTATION - BATCHES ---------------\n",
      "ITERATION : 0\n",
      "START_BATCH: 230, END_BATCH: 245, NEXT_CHANGE: 302\n",
      "ITERATION : 1\n",
      "START_BATCH: 302, END_BATCH: 317, NEXT_CHANGE: 358\n",
      "ITERATION : 2\n",
      "START_BATCH: 358, END_BATCH: 373, NEXT_CHANGE: 416\n",
      "ITERATION : 3\n",
      "START_BATCH: 416, END_BATCH: 431, NEXT_CHANGE: 444\n",
      "ITERATION : 4\n",
      "START_BATCH: 444, END_BATCH: 459, NEXT_CHANGE: 578\n",
      "ITERATION : 5\n",
      "START_BATCH: 578, END_BATCH: 593, NEXT_CHANGE: 654\n",
      "ITERATION : 6\n",
      "START_BATCH: 654, END_BATCH: 669, NEXT_CHANGE: 816\n",
      "ITERATION : 7\n",
      "START_BATCH: 816, END_BATCH: 831, NEXT_CHANGE: 1000\n"
     ]
    }
   ],
   "source": [
    "batch_size=15\n",
    "name = \"SEA_A\"\n",
    "print(name)\n",
    "X = all_synthetic[name]\n",
    "x_cols = X.columns.values[:-1]\n",
    "print(x_cols)\n",
    "\n",
    "stream_change_detected, X_det =  distribution_drift_detection(X.copy(),\n",
    "                                 referance_samples = 50, \n",
    "                                 x_cols = x_cols,\n",
    "                                 min_instances = 1,\n",
    "                                 samples_to_check = 2,\n",
    "                                 delta = 0.005,\n",
    "                                 threshold = 10,\n",
    "                                 alpha = 0.9999,\n",
    "                                 save = False\n",
    "                                 \n",
    "                                )\n",
    "\n",
    "def get_encoder(input_shape=(X[x_cols].shape[1],)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu',\n",
    "                        input_shape=input_shape))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(2, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.00001), loss=BinaryFocalLoss(gamma=10))\n",
    "    return model\n",
    "\n",
    "def get_task(input_shape=(X[x_cols].shape[1],)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.00001), loss=BinaryFocalLoss(gamma=10))\n",
    "    return model\n",
    "    \n",
    "def get_discriminator(input_shape=(X[x_cols].shape[1],)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryFocalLoss(gamma=2))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "ccsa_enc = get_encoder()\n",
    "ccsa_disc= get_discriminator()\n",
    "ccsa_task = get_task()\n",
    "\n",
    "\n",
    "all_predictions_HTC, all_predictions_HATC, all_predictions_adwin, all_predictions_knn = streaming_learning(X, \n",
    "                                                               model_1 = HoeffdingTreeClassifier(),  \n",
    "                                                               model_2 = HoeffdingAdaptiveTreeClassifier(), \n",
    "                                                               model_3 = ensemble.ADWINBaggingClassifier(model=linear_model.LogisticRegression()),\n",
    "                                                               model_4 = neighbors.KNNClassifier(),\n",
    "                                                               x_cols = x_cols, \n",
    "                                                               y_col = [\"class\"])\n",
    "\n",
    "\n",
    "result_batches, predictions = stream_domain_adaptation(X=X, \n",
    "                                                 ccsa_enc=ccsa_enc, \n",
    "                                                 ccsa_task=ccsa_task, \n",
    "                                                 ccsa_disc=ccsa_disc, \n",
    "                                                 all_predictions_stream_HTC=all_predictions_HTC, \n",
    "                                                 all_predictions_stream_HATC=all_predictions_HATC, \n",
    "                                                 all_predictions_adwin=all_predictions_adwin, \n",
    "                                                 all_predictions_knn=all_predictions_knn, \n",
    "                                                 stream_change=stream_change_detected, \n",
    "                                                 batch_size = batch_size, \n",
    "                                                 x_cols = x_cols,\n",
    "                                                 y_col = [\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ff7fc68-4623-416f-a393-772ebcab7cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_DA</th>\n",
       "      <th>f1_HTC</th>\n",
       "      <th>f1_HATC</th>\n",
       "      <th>f1_knn</th>\n",
       "      <th>f1_adwin</th>\n",
       "      <th>G_da</th>\n",
       "      <th>G_HTC</th>\n",
       "      <th>G_HATC</th>\n",
       "      <th>G_knn</th>\n",
       "      <th>G_adwin</th>\n",
       "      <th>accuracy_da</th>\n",
       "      <th>accuracy_HTC</th>\n",
       "      <th>accuracy_HATC</th>\n",
       "      <th>accuracy_knn</th>\n",
       "      <th>accuracy_adwin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.916968</td>\n",
       "      <td>0.895662</td>\n",
       "      <td>0.898007</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.80947</td>\n",
       "      <td>0.873586</td>\n",
       "      <td>0.827535</td>\n",
       "      <td>0.830929</td>\n",
       "      <td>0.850336</td>\n",
       "      <td>0.662301</td>\n",
       "      <td>0.893846</td>\n",
       "      <td>0.863077</td>\n",
       "      <td>0.866154</td>\n",
       "      <td>0.875385</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1_DA    f1_HTC   f1_HATC    f1_knn  f1_adwin      G_da     G_HTC  \\\n",
       "0  0.916968  0.895662  0.898007  0.903226   0.80947  0.873586  0.827535   \n",
       "\n",
       "     G_HATC     G_knn   G_adwin  accuracy_da  accuracy_HTC  accuracy_HATC  \\\n",
       "0  0.830929  0.850336  0.662301     0.893846      0.863077       0.866154   \n",
       "\n",
       "   accuracy_knn  accuracy_adwin  \n",
       "0      0.875385            0.74  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_res = predictions.replace({True: 1, False: 0})\n",
    "scores = {\"f1_DA\" : [f1_score(df_res[\"y_true\"], df_res[\"y_pred_da\"])],\n",
    "          \"f1_HTC\" : [f1_score(df_res[\"y_true\"], df_res[\"y_pred_stream_HTC\"])],\n",
    "          \"f1_HATC\" : [f1_score(df_res[\"y_true\"], df_res[\"y_pred_stream_HATC\"])],\n",
    "          \"f1_knn\" : [f1_score(df_res[\"y_true\"], df_res[\"y_pred_stream_knn\"])],\n",
    "          \"f1_adwin\" : [f1_score(df_res[\"y_true\"], df_res[\"y_pred_stream_adwin\"])],\n",
    "          \"G_da\" : [geometric_mean_score(df_res[\"y_true\"], df_res[\"y_pred_da\"], average='binary')],\n",
    "          \"G_HTC\" : [geometric_mean_score(df_res[\"y_true\"], df_res[\"y_pred_stream_HTC\"], average='binary')],         \n",
    "          \"G_HATC\" : [geometric_mean_score(df_res[\"y_true\"], df_res[\"y_pred_stream_HATC\"], average='binary')],\n",
    "          \"G_knn\" : [geometric_mean_score(df_res[\"y_true\"], df_res[\"y_pred_stream_knn\"], average='binary')],\n",
    "          \"G_adwin\" : [geometric_mean_score(df_res[\"y_true\"], df_res[\"y_pred_stream_adwin\"], average='binary')],\n",
    "          \"accuracy_da\" : [accuracy_score(df_res[\"y_true\"], df_res[\"y_pred_da\"])],\n",
    "          \"accuracy_HTC\" : [accuracy_score(df_res[\"y_true\"], df_res[\"y_pred_stream_HTC\"])],         \n",
    "          \"accuracy_HATC\" : [accuracy_score(df_res[\"y_true\"], df_res[\"y_pred_stream_HATC\"])],\n",
    "          \"accuracy_knn\" : [accuracy_score(df_res[\"y_true\"], df_res[\"y_pred_stream_knn\"])],\n",
    "          \"accuracy_adwin\" : [accuracy_score(df_res[\"y_true\"], df_res[\"y_pred_stream_adwin\"])],\n",
    "                         }\n",
    "display(pd.DataFrame(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "domain_adapt_env",
   "language": "python",
   "name": "domain_adapt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
